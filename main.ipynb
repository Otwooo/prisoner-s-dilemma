{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import Env \n",
    "import numpy as np\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 환경 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def All_Cooperate(state, game_step): # 항상 협력\n",
    "    return 1\n",
    "\n",
    "def All_Cheat(state, game_step): # 항상 배신\n",
    "    return 0\n",
    "\n",
    "def Copycat(state, game_step): # 처음에는 협력 이후, 상대방의 전 전략을 따라함\n",
    "    if game_step == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return state[game_step-2]\n",
    "    \n",
    "def Grudger(state, game_step): # 항상 협력, 하지만 상대방이 한번이라도 배신하면 끝까지 배신\n",
    "    for i in range(0, game_step-2+1, 2):\n",
    "        print(i)\n",
    "        if state[i] == 0:            \n",
    "            return 0\n",
    "    return 1\n",
    "\n",
    "def Detective(state, game_step): # 1-0-1-1으로 시작하고, 이 때 상대가 한번이라도 배신하면 Copycat 방식, 그렇지 않으면 All_Cheat 방식을 따른다.\n",
    "    start = [1, -1, 0, -1, 1, -1, 1]\n",
    "    if game_step <= 6:\n",
    "        return start[game_step]\n",
    "    \n",
    "    for i in range(0, 7, 2):\n",
    "        if state[i] == 0:\n",
    "            return Copycat(state, game_step)\n",
    "    return All_Cheat(state, game_step)\n",
    "\n",
    "def Copykitten(state, game_step): # 첫번 째는 협력으로 시작한다. 상대가 두번연속 배신하면 배신한다.\n",
    "    if game_step == 0 or game_step == 2:\n",
    "        return 1\n",
    "    elif state[game_step-2] == 0 and state[game_step-4] == 0:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "def Simpleton(state, game_step): # 첫번 째는 협력으로 시작한다. 상대가 협력하면 내 마지막 수와 같은 수를, 상대가 배신하면 내 마지막 수와 다른 수를 둔다.\n",
    "    if game_step == 0:\n",
    "        return 1\n",
    "    elif state[game_step-2] == 1:\n",
    "        return state[game_step-1]\n",
    "    elif state[game_step-2] == 0:\n",
    "        if state[game_step-1] == 1:\n",
    "            return 0\n",
    "        if state[game_step-1] == 0:\n",
    "            return 1\n",
    "        \n",
    "def Random_Game(state, game_step):\n",
    "    return random.choice([0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrisonersGame(Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.action_space = Discrete(3)\n",
    "        self.observation_space = Box(low=-1, high=1, shape=(30,), dtype=np.int8)\n",
    "\n",
    "        self.state = [-1 for i in range(30)]\n",
    "        self.game_step = 0\n",
    "\n",
    "        self.Opponent_Game_type = [\"All_Cooperate\", \"All_Cheat\", \"Copycat\", \"Grudger\", \"Detective\", \"Copykitten\", \"Simpleton\", \"Random_Game\"]        \n",
    "    \n",
    "    def get_observation(self):        \n",
    "        return self.state\n",
    "    \n",
    "    def get_done(self):\n",
    "        done=False\n",
    "        if self.state[29] != -1:\n",
    "            done = True\n",
    "        return done\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = [-1 for i in range(30)]\n",
    "        self.game_step = 0\n",
    "        return self.get_observation\n",
    "    \n",
    "    def step(self, action):\n",
    "\n",
    "        self.state[self.game_step] = action\n",
    "        self.state[self.game_step+1] = eval(self.Opponent_Game_type[0])(self.state, self.game_step)\n",
    "        \n",
    "        reward = 0\n",
    "        if self.state[self.game_step] == 1: # 나는 협력을 했는데\n",
    "            if self.state[self.game_step+1] == 1: # 상대도 협력\n",
    "                reward = +2\n",
    "            elif self.state[self.game_step+1] == 1: # 상대는 배신\n",
    "                reward = -1\n",
    "        if self.state[self.game_step] == 0: # 나는 배신을 했는데\n",
    "            if self.state[self.game_step+1] == 1: # 상대는 협력\n",
    "                reward = +3\n",
    "            elif self.state[self.game_step+1] == 0: # 상대도 배신\n",
    "                reward = 0\n",
    "        \n",
    "        self.game_step += 2\n",
    "        \n",
    "        observation = self.get_observation()\n",
    "        done = self.get_done() \n",
    "        info = {}\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PrisonersGame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m obs\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39;49mget_observation()\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(obs)\n",
      "Cell \u001b[0;32mIn[80], line 15\u001b[0m, in \u001b[0;36mPrisonersGame.get_observation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_observation\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     14\u001b[0m     info \u001b[39m=\u001b[39m {}\n\u001b[0;32m---> 15\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, info}\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "obs=env.get_observation()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m total_reward   \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done: \n\u001b[0;32m----> 6\u001b[0m     obs, reward,  done, info \u001b[39m=\u001b[39m  env\u001b[39m.\u001b[39;49mstep(env\u001b[39m.\u001b[39;49maction_space\u001b[39m.\u001b[39;49msample())\n\u001b[1;32m      7\u001b[0m     total_reward  \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTotal Reward for episode \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(episode, total_reward))    \n",
      "Cell \u001b[0;32mIn[80], line 47\u001b[0m, in \u001b[0;36mPrisonersGame.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     43\u001b[0m         reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m---> 47\u001b[0m observation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_observation()\n\u001b[1;32m     48\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_done() \n\u001b[1;32m     49\u001b[0m info \u001b[39m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[80], line 15\u001b[0m, in \u001b[0;36mPrisonersGame.get_observation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_observation\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     14\u001b[0m     info \u001b[39m=\u001b[39m {}\n\u001b[0;32m---> 15\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, info}\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "for episode in range(10): \n",
    "    obs = env.reset()\n",
    "    done = False  \n",
    "    total_reward   = 0\n",
    "    while not done: \n",
    "        obs, reward,  done, info =  env.step(env.action_space.sample())\n",
    "        total_reward  += reward\n",
    "    print('Total Reward for episode {} is {}'.format(episode, total_reward))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "from gymnasium.utils.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baesonghyeon/Documents/Project/prisoner's dilemma/venv/lib/python3.11/site-packages/gymnasium/utils/env_checker.py:270: UserWarning: \u001b[33mWARN: `check_env(warn=...)` parameter is now ignored.\u001b[0m\n",
      "  logger.warn(\"`check_env(warn=...)` parameter is now ignored.\")\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'method'>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[39m=\u001b[39m PrisonersGame()\n\u001b[0;32m----> 2\u001b[0m check_env(env , warn \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m , skip_render_check \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m )\n",
      "File \u001b[0;32m~/Documents/Project/prisoner's dilemma/venv/lib/python3.11/site-packages/gymnasium/utils/env_checker.py:297\u001b[0m, in \u001b[0;36mcheck_env\u001b[0;34m(env, warn, skip_render_check)\u001b[0m\n\u001b[1;32m    295\u001b[0m check_seed_deprecation(env)\n\u001b[1;32m    296\u001b[0m check_reset_return_info_deprecation(env)\n\u001b[0;32m--> 297\u001b[0m check_reset_return_type(env)\n\u001b[1;32m    298\u001b[0m check_reset_seed(env)\n\u001b[1;32m    299\u001b[0m check_reset_options(env)\n",
      "File \u001b[0;32m~/Documents/Project/prisoner's dilemma/venv/lib/python3.11/site-packages/gymnasium/utils/env_checker.py:201\u001b[0m, in \u001b[0;36mcheck_reset_return_type\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Checks that :meth:`reset` correctly returns a tuple of the form `(obs , info)`.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39m    AssertionError depending on spec violation\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m result \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m--> 201\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m    202\u001b[0m     result, \u001b[39mtuple\u001b[39m\n\u001b[1;32m    203\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(result)\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    205\u001b[0m     \u001b[39mlen\u001b[39m(result) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    206\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCalling the reset method did not return a 2-tuple, actual length: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(result)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m obs, info \u001b[39m=\u001b[39m result\n",
      "\u001b[0;31mAssertionError\u001b[0m: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'method'>`"
     ]
    }
   ],
   "source": [
    "env = PrisonersGame()\n",
    "check_env(env , warn = True , skip_render_check = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "You should use NatureCNN only with images not with Box(-1, 1, (30,), int8)\n(you are probably using `CnnPolicy` instead of `MlpPolicy` or `MultiInputPolicy`)\nIf you are using a custom environment,\nplease check it using our env checker:\nhttps://stable-baselines3.readthedocs.io/en/master/common/env_checker.html.\nIf you are using `VecNormalize` or already normalized channel-first images you should pass `normalize_images=False`: \nhttps://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[39m=\u001b[39m PrisonersGame()\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m DQN(\u001b[39m'\u001b[39;49m\u001b[39mCnnPolicy\u001b[39;49m\u001b[39m'\u001b[39;49m, env, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, buffer_size\u001b[39m=\u001b[39;49m\u001b[39m1200000\u001b[39;49m, learning_starts\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m model\u001b[39m.\u001b[39mlearn(total_timesteps\u001b[39m=\u001b[39m\u001b[39m100000\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Project/prisoner's dilemma/venv/lib/python3.11/site-packages/stable_baselines3/dqn/dqn.py:141\u001b[0m, in \u001b[0;36mDQN.__init__\u001b[0;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, target_update_interval, exploration_fraction, exploration_initial_eps, exploration_final_eps, max_grad_norm, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexploration_rate \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m    140\u001b[0m \u001b[39mif\u001b[39;00m _init_setup_model:\n\u001b[0;32m--> 141\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_model()\n",
      "File \u001b[0;32m~/Documents/Project/prisoner's dilemma/venv/lib/python3.11/site-packages/stable_baselines3/dqn/dqn.py:144\u001b[0m, in \u001b[0;36mDQN._setup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_setup_model\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_setup_model()\n\u001b[1;32m    145\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_aliases()\n\u001b[1;32m    146\u001b[0m     \u001b[39m# Copy running stats, see GH issue #996\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Project/prisoner's dilemma/venv/lib/python3.11/site-packages/stable_baselines3/common/off_policy_algorithm.py:199\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._setup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m         replay_buffer_kwargs[\u001b[39m\"\u001b[39m\u001b[39menv\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\n\u001b[1;32m    189\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplay_buffer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplay_buffer_class(\n\u001b[1;32m    190\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_size,\n\u001b[1;32m    191\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mreplay_buffer_kwargs,  \u001b[39m# pytype:disable=wrong-keyword-args\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     )\n\u001b[0;32m--> 199\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_class(  \u001b[39m# pytype:disable=not-instantiable\u001b[39;49;00m\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobservation_space,\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_space,\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlr_schedule,\n\u001b[1;32m    203\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_kwargs,  \u001b[39m# pytype:disable=not-instantiable\u001b[39;49;00m\n\u001b[1;32m    204\u001b[0m )\n\u001b[1;32m    205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    207\u001b[0m \u001b[39m# Convert train freq parameter to TrainFreq object\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Project/prisoner's dilemma/venv/lib/python3.11/site-packages/stable_baselines3/dqn/policies.py:248\u001b[0m, in \u001b[0;36mCnnPolicy.__init__\u001b[0;34m(self, observation_space, action_space, lr_schedule, net_arch, activation_fn, features_extractor_class, features_extractor_kwargs, normalize_images, optimizer_class, optimizer_kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    236\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    237\u001b[0m     observation_space: spaces\u001b[39m.\u001b[39mSpace,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m     optimizer_kwargs: Optional[Dict[\u001b[39mstr\u001b[39m, Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    247\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 248\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    249\u001b[0m         observation_space,\n\u001b[1;32m    250\u001b[0m         action_space,\n\u001b[1;32m    251\u001b[0m         lr_schedule,\n\u001b[1;32m    252\u001b[0m         net_arch,\n\u001b[1;32m    253\u001b[0m         activation_fn,\n\u001b[1;32m    254\u001b[0m         features_extractor_class,\n\u001b[1;32m    255\u001b[0m         features_extractor_kwargs,\n\u001b[1;32m    256\u001b[0m         normalize_images,\n\u001b[1;32m    257\u001b[0m         optimizer_class,\n\u001b[1;32m    258\u001b[0m         optimizer_kwargs,\n\u001b[1;32m    259\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Project/prisoner's dilemma/venv/lib/python3.11/site-packages/stable_baselines3/dqn/policies.py:151\u001b[0m, in \u001b[0;36mDQNPolicy.__init__\u001b[0;34m(self, observation_space, action_space, lr_schedule, net_arch, activation_fn, features_extractor_class, features_extractor_kwargs, normalize_images, optimizer_class, optimizer_kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_fn \u001b[39m=\u001b[39m activation_fn\n\u001b[1;32m    143\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet_args \u001b[39m=\u001b[39m {\n\u001b[1;32m    144\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mobservation_space\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space,\n\u001b[1;32m    145\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39maction_space\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mnormalize_images\u001b[39m\u001b[39m\"\u001b[39m: normalize_images,\n\u001b[1;32m    149\u001b[0m }\n\u001b[0;32m--> 151\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build(lr_schedule)\n",
      "File \u001b[0;32m~/Documents/Project/prisoner's dilemma/venv/lib/python3.11/site-packages/stable_baselines3/dqn/policies.py:163\u001b[0m, in \u001b[0;36mDQNPolicy._build\u001b[0;34m(self, lr_schedule)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_build\u001b[39m(\u001b[39mself\u001b[39m, lr_schedule: Schedule) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39m    Create the network and the optimizer.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39m        lr_schedule(1) is the initial learning rate\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_net \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_q_net()\n\u001b[1;32m    164\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_net_target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_q_net()\n\u001b[1;32m    165\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_net_target\u001b[39m.\u001b[39mload_state_dict(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_net\u001b[39m.\u001b[39mstate_dict())\n",
      "File \u001b[0;32m~/Documents/Project/prisoner's dilemma/venv/lib/python3.11/site-packages/stable_baselines3/dqn/policies.py:177\u001b[0m, in \u001b[0;36mDQNPolicy.make_q_net\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_q_net\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m QNetwork:\n\u001b[1;32m    176\u001b[0m     \u001b[39m# Make sure we always have separate networks for features extractors etc\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     net_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_features_extractor(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet_args, features_extractor\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    178\u001b[0m     \u001b[39mreturn\u001b[39;00m QNetwork(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnet_args)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/Documents/Project/prisoner's dilemma/venv/lib/python3.11/site-packages/stable_baselines3/common/policies.py:114\u001b[0m, in \u001b[0;36mBaseModel._update_features_extractor\u001b[0;34m(self, net_kwargs, features_extractor)\u001b[0m\n\u001b[1;32m    111\u001b[0m net_kwargs \u001b[39m=\u001b[39m net_kwargs\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m features_extractor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[39m# The features extractor is not shared, create a new one\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     features_extractor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_features_extractor()\n\u001b[1;32m    115\u001b[0m net_kwargs\u001b[39m.\u001b[39mupdate(\u001b[39mdict\u001b[39m(features_extractor\u001b[39m=\u001b[39mfeatures_extractor, features_dim\u001b[39m=\u001b[39mfeatures_extractor\u001b[39m.\u001b[39mfeatures_dim))\n\u001b[1;32m    116\u001b[0m \u001b[39mreturn\u001b[39;00m net_kwargs\n",
      "File \u001b[0;32m~/Documents/Project/prisoner's dilemma/venv/lib/python3.11/site-packages/stable_baselines3/common/policies.py:120\u001b[0m, in \u001b[0;36mBaseModel.make_features_extractor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_features_extractor\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseFeaturesExtractor:\n\u001b[1;32m    119\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Helper method to create a features extractor.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures_extractor_class(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobservation_space, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures_extractor_kwargs)\n",
      "File \u001b[0;32m~/Documents/Project/prisoner's dilemma/venv/lib/python3.11/site-packages/stable_baselines3/common/torch_layers.py:77\u001b[0m, in \u001b[0;36mNatureCNN.__init__\u001b[0;34m(self, observation_space, features_dim, normalized_image)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(observation_space, features_dim)\n\u001b[1;32m     75\u001b[0m \u001b[39m# We assume CxHxW images (channels first)\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39m# Re-ordering will be done by pre-preprocessing or wrapper\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m \u001b[39massert\u001b[39;00m is_image_space(observation_space, check_channels\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, normalized_image\u001b[39m=\u001b[39mnormalized_image), (\n\u001b[1;32m     78\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mYou should use NatureCNN \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39monly with images not with \u001b[39m\u001b[39m{\u001b[39;00mobservation_space\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m(you are probably using `CnnPolicy` instead of `MlpPolicy` or `MultiInputPolicy`)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mIf you are using a custom environment,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mplease check it using our env checker:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mhttps://stable-baselines3.readthedocs.io/en/master/common/env_checker.html.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mIf you are using `VecNormalize` or already normalized channel-first images \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39myou should pass `normalize_images=False`: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mhttps://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m )\n\u001b[1;32m     88\u001b[0m n_input_channels \u001b[39m=\u001b[39m observation_space\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     89\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcnn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m     90\u001b[0m     nn\u001b[39m.\u001b[39mConv2d(n_input_channels, \u001b[39m32\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m),\n\u001b[1;32m     91\u001b[0m     nn\u001b[39m.\u001b[39mReLU(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m     nn\u001b[39m.\u001b[39mFlatten(),\n\u001b[1;32m     97\u001b[0m )\n",
      "\u001b[0;31mAssertionError\u001b[0m: You should use NatureCNN only with images not with Box(-1, 1, (30,), int8)\n(you are probably using `CnnPolicy` instead of `MlpPolicy` or `MultiInputPolicy`)\nIf you are using a custom environment,\nplease check it using our env checker:\nhttps://stable-baselines3.readthedocs.io/en/master/common/env_checker.html.\nIf you are using `VecNormalize` or already normalized channel-first images you should pass `normalize_images=False`: \nhttps://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html"
     ]
    }
   ],
   "source": [
    "env = PrisonersGame()\n",
    "model = DQN('CnnPolicy', env, verbose=1, buffer_size=1200000, learning_starts=1000)\n",
    "\n",
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
